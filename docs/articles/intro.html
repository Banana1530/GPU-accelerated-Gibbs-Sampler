<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>GPU-accelerated Gibbs Sampler • GAGS </title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="GPU-accelerated Gibbs Sampler">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-000000-01"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-000000-01');
</script>
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../articles/intro.html">GAGS </a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">0.0.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/intro.html">Home</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/Banana1530/GPU-accelerated-Gibbs-Sampler">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>GPU-accelerated Gibbs Sampler</h1>
                        <h4 class="author">Luofeng Liao</h4>
            
            <h4 class="date">2019-09-05</h4>
      
      
      <div class="hidden name"><code>intro.Rmd</code></div>

    </div>

    
    
<div id="motivation" class="section level2">
<h2 class="hasAnchor">
<a href="#motivation" class="anchor"></a>Motivation</h2>
<p>The Bayesian statistical paradigm is particular desirable in the big-data setting, because it allows us to impose prior knowledge on the data and produces a posterior distribution that summarizes knowledge from both data and humans. However, it is inherently computation-heavy. The advance of parallel computing sheds new light upon Bayesian inference. In <span class="citation">(Lee et al. 2010)</span> the author summarizes some efficient examples of parallel Bayesian methods. Inspired by this, this project implements the GPU version of the hierarchical models in <span class="citation">(Makalic and Schmidt 2016)</span>, which features sparse estimates of the regression coefficients and is robust to non-Gaussian noise. The tremendous speed-up obtained by GPU shows the prospect of large-scale Bayesian inference.</p>
</div>
<div id="model-description" class="section level2">
<h2 class="hasAnchor">
<a href="#model-description" class="anchor"></a>Model Description</h2>
<p>Bayesian inference is a “simple” trilogy:</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>Specify priors;</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>Based on likelihood, calculate the posterior;</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>Sample from the posterior. Of course model checking is required for a specific data set.</li>
</ol></li>
</ul>
<p>The hierarchical Bayesian model proposed in <span class="citation">Makalic and Schmidt (2016)</span> is interesting in that it features continuous shrinkage prior densities and heavy-tailed error models. Formally, for responses <span class="math inline">\(z = \{z_1, \cdots, z_n \}\)</span> and data matrix <span class="math inline">\(X = (x_1,\cdots, x_n)^T \in \mathbb{R}^{n \times p}\)</span> of <span class="math inline">\(p\)</span> predictors, we construct the following hierarchical model.</p>
<center>
<img src="graphical.png" style="width: 40%" title="Graphical Representation"><br>
</center>
<p>The model consists of two major components:</p>
<ul>
<li>the sampling distribution of the data, <span class="math inline">\(z\)</span>, and</li>
<li>the prior distribution imposed on the regression coefficients <span class="math inline">\(\beta\)</span>.</li>
</ul>
<p>The scale parameter <span class="math inline">\(\sigma^2\)</span> is given the scale invariant prior <span class="math inline">\(\pi(\sigma^2) \propto 1/\sigma^2\)</span>, the intercept term <span class="math inline">\(\beta_0\)</span> the uniform prior, and the global shrinkage parameter <span class="math inline">\(\tau\)</span> the half-Cauchy distribution <span class="math inline">\(C^+(0,1)\)</span>, i.e., <span class="math inline">\(\pi(\tau) = 2/\pi(1+\tau^2),\tau&gt;0\)</span>.</p>
<p>Below is an overview of the project.</p>
<center>
<img src="prior.png" style="width: 80%" title="Comparison"><br>
</center>
<div id="heavy-tailed-error-distributions" class="section level3">
<h3 class="hasAnchor">
<a href="#heavy-tailed-error-distributions" class="anchor"></a>Heavy-tailed error distributions</h3>
<p>The scale parameter <span class="math inline">\(\sigma^2&gt;0\)</span> and the latent variables <span class="math inline">\(\{\omega_1,\cdots,\omega_n \}\)</span> are used to model a variety of error distributions, that is, the distribution of <span class="math inline">\(e_i = z_i - \beta_0 - X_{i\cdot}\beta\)</span> given all other variables.</p>
<p>As a reminder, we call <span class="math inline">\(Z\)</span> follows a Gaussian scale mixture if it can be generated by <span class="math display">\[Z|\lambda \sim \mathcal{N}(\mu,g(\lambda)\sigma^2), \lambda \sim \pi(\lambda)d\lambda.\]</span> Different choices of <span class="math inline">\(\lambda\)</span> and the mixing density <span class="math inline">\(g(\lambda)\)</span> result in various non-Gaussian distributions of <span class="math inline">\(Z\)</span>.</p>
<p>In this project, we plan to provide the choices listed in the following table. Note <span class="math inline">\(\operatorname{E}\)</span> is the exponential distribution, <span class="math inline">\(\operatorname{IG}\)</span> is the inverse-Gamma distribution.</p>
<ul>
<li>
<span class="math inline">\(\omega_i = 1\)</span> , <span class="math inline">\(e_i\)</span>’s are Gaussian errors (DONE)</li>
<li>
<span class="math inline">\(\omega_i \sim \operatorname{E}(1)\)</span> , <span class="math inline">\(e_i\)</span>’s are Laplace errors (DONE)</li>
<li>
<span class="math inline">\(\omega_i \sim \operatorname{IG}(\delta/2,\delta/2)\)</span> , <span class="math inline">\(e_i\)</span>’s are Student-<span class="math inline">\(t\)</span> of df <span class="math inline">\(\delta &gt; 2\)</span> (TODO)</li>
</ul>
</div>
<div id="sparsity-inducing-priors" class="section level3">
<h3 class="hasAnchor">
<a href="#sparsity-inducing-priors" class="anchor"></a>Sparsity-inducing priors</h3>
<p>The hyperparameters <span class="math inline">\(\tau^2\)</span> and <span class="math inline">\((\lambda_1,\cdots, \lambda_p)\)</span> are used to model the sparsity in the coefficients and act as globa and local shrinkage. In other words, we care about the distribution of <span class="math inline">\(\beta\)</span> given <span class="math inline">\(\tau^2\)</span> and <span class="math inline">\((\lambda_1,\cdots, \lambda_p)\)</span>.</p>
<p>Currently our project provides the following three options in Tab.2. The Horseshoe estimator is relatively exotic. The general idea is to construct a prior on <span class="math inline">\(\beta\)</span> that has an infinitely tall spike at 0 and heavy, Cauchy-like tails that decay like <span class="math inline">\(f(x)=1/x^2\)</span>.</p>
<ul>
<li>
<span class="math inline">\(\lambda _ { j } ^ { 2 } = 1\)</span>, Ridge regression (TODO)</li>
<li>
<span class="math inline">\(\lambda _ { j } ^ { 2 } \sim \operatorname { E } ( 1 )\)</span>, The lasso (DONE)</li>
<li>
<span class="math inline">\(\lambda _ { j } \sim { C } ^ { + } ( 0,1 )\)</span> , The Horseshoe estimator (TODO)</li>
</ul>
</div>
</div>
<div id="a-simulation-study" class="section level2">
<h2 class="hasAnchor">
<a href="#a-simulation-study" class="anchor"></a>A Simulation Study</h2>
<p>This part follows closely the simulation section of <span class="citation">(Makalic and Schmidt 2016)</span>.</p>
<p>We construct a data set <span class="math inline">\(X \in \mathbb{R}^{ 50 \times 10}\)</span> with <span class="math inline">\({Cov}(X_{\cdot i},X_{\cdot j}) = 0.5^{|i-j|}\)</span> and <span class="math inline">\(\beta \in \mathbb{R}^{10}\)</span> with only four non-zero entries. <span class="math inline">\(y_i = \beta_0 + X_{i\cdot}\beta + e_i\)</span>, where <span class="math inline">\(e_i\)</span> is Laplace noise. Then we run both lasso (the left image) and the Bayesian regression with Laplace errors (the right one) and the Horseshoe type sparsity to discover the non-zero coefficients. The advantages of the model are (i) absence of hyperparameters and (ii) relatively more accurate estimates.</p>
<center>
<img src="lassohorse.png" style="width: 100%" title="Lasso"><br>
</center>
</div>
<div id="the-parallel-gibbs-sampler" class="section level2">
<h2 class="hasAnchor">
<a href="#the-parallel-gibbs-sampler" class="anchor"></a>The Parallel Gibbs Sampler</h2>
<p>One of the most commonly used methods to sample from the posterior distribution is the Gibbs sampler. To sample from a multivariate distribution, for example, <span class="math inline">\(f(x_1,x_2,x_3)\)</span>, we can sample from the following conditionals iteratively.</p>
<span class="math display">\[\begin{align}
    x_1^{k+1}\sim f_{1|2,3}(\cdot|x_2^k,x_3^k),\,\,
      x_2^{k+1} \sim f_{2|1,3}(\cdot|x_1^{k+1},x_3^k),\,\,
      x_3^{k+1} \sim f_{3|1,2}(\cdot|x_1^{k+1},x_2^{k+1}),\,
      \cdots
\end{align}\]</span>
<p>For example, the model with Student-<span class="math inline">\(t\)</span> errors and <span class="math inline">\(\lambda _ { j } ^ { 2 } \sim \operatorname { E } ( 1 )\)</span> has the following set of full conditionals (we suppress notations for “given all other variables”):</p>
<span class="math display">\[\begin{align}
  \beta &amp;\sim \mathcal { N } _ { p } \left( \tilde {  { \mu } } ,  { A } _ { p } ^ { - 1 } \right), 1/{\xi}  \sim \operatorname{E}(1+{1}/{\tau^2}), 1/\nu_j \sim \operatorname{E}(1+{1}/{\lambda_j })
  \\
  \omega_i^2 &amp;\sim \mathrm { IG } \big( \frac { \delta + 1 } { 2 } , \frac { 1 } { 2 } (  { e _ { i } ^ { 2 } } / { \sigma ^ { 2 } } + \delta ) \big) ,
  \lambda_j \sim \operatorname{E}( { 1 }/ { \nu _ { j } } +  { \beta _ { j } ^ { 2 } }/ { 2 \tau ^ { 2 } \sigma ^ { 2 } })
  \\
  \sigma^2 &amp;\sim \mathrm { IG } \big( \frac { n + p } { 2 } , \frac { 1 } { 2 } ( \sum _ { i = 1 } ^ { n }  { e _ { i } ^ { 2 } }/ { \omega _ { i } ^ { 2 } } + \sum _ { j = 1 } ^ { p }  { \beta _ { j } ^ { 2 } } / { \tau ^ { 2 } \lambda _ { j } ^ { 2 } } ) \big),
  \\
  \tau^2  &amp;\sim  \operatorname{IG}(\frac{p+1}{2},   { 1 }/ { \xi } + \frac { 1 } { 2 \sigma ^ { 2 } } \sum _ { j = 1 } ^ { p }  { \beta _ { j } ^ { 2 } } / { \lambda _ { j } ^ { 2 } })

\end{align}\]</span>
<p>where <span class="math inline">\(j = 1,\cdots,n,i = 1,\cdots, p\)</span>. <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\nu_j\)</span> are auxiliary variables.</p>
<div id="conditional-independence-in-the-posterior-distributions" class="section level3">
<h3 class="hasAnchor">
<a href="#conditional-independence-in-the-posterior-distributions" class="anchor"></a>Conditional independence in the posterior distributions</h3>
<p>We <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are conditionally independent given <span class="math inline">\(z\)</span> if <span class="math inline">\(p(x,y|z) = p(x|z)p(y|z)\)</span>. In the above conditionals, variables listed in the same line are conditionally independent, and therefore can be sampled simultaneously. See Fig.3 for demostration. This is particularly important in the big-data scenario. For large <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, sampling sequentially for <span class="math inline">\(\nu_j,\lambda_j,\omega_i\)</span> would be expensive.</p>
<p>Thd following graph illustrates the sampling schemes. From top to bottom are the streams for sampling and collecting <span class="math inline">\((\beta,\xi,\nu)\)</span>, <span class="math inline">\((\omega,\lambda)\)</span>, <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\tau\)</span> and finally an independent stream generating a lot of uniforms.</p>
<center>
<img src="sampling.png" title="Sampling Scheme" style="width: 30%"><br>
</center>
</div>
</div>
<div id="gpu-and-cuda" class="section level2">
<h2 class="hasAnchor">
<a href="#gpu-and-cuda" class="anchor"></a>GPU and CUDA</h2>
<p>GPU is well suited for SIMD (Single Instruction Multiple Data) parallelization, i.e., tasks that can be partitioned into many parts and there is little dependence across sub-tasks. In CUDA terminology, when we start a task (summing two matrices, for example), we launch a , and  and  (Fig. 4) determine the way we partition the task. See the following graph <span class="citation">(Kirk and Wen-Mei 2016)</span>.</p>
<center>
<img src="gpuarchi.png" style="width: 25%">
</center>
<p>The following techniques are crucial in the sampling from the posterior:</p>
<ul>
<li><p>cuBLAS: parallel linear algebra subroutines. Sampling from high-D Gaussian is the very bottle-neck of the algorithm, because we need to draw a vector from a <span class="math inline">\(p\)</span>-dimensional Gaussian that changes at every iteration. Most techniques that I know of are based on matrix factorization, e.g., Cholesky, QR or SVD. This can be done in parallel using the well-tuned CUDA package cuBLAS.</p></li>
<li><p>Streaming. Streaming allows us to execute multiple kernels at the same time, if GPU resources allow. (a) We launch multiple threads to sample <span class="math inline">\(\nu_1,\cdots, \nu_p\)</span>, and launch multiple kernels to sample <span class="math inline">\(\beta,\xi,\nu\)</span> in parallel. (b) Generate uniforms concurrently. Many sampling methods in the model are inverse transformation. We can first pre-generate lots of uniforms, and then refill it when it is used up. All these can be done concurrently with the sampling.</p></li>
<li><p>Asynchronous data transfer. We can execute the program and transfer data back to CPU at the same time. This can be applied in gathering draws from the posterior while keep the sampler running. Due to the expense of CPU-GPU data transfer, this could save a lot time.</p></li>
<li><p>Thrust: building-block parallel algorithms. Thrust is a CUDA library that provides the parallel version of C++ Standard Template Library (STL). Moreover, Thrust provides a rich collection of data parallel primitives such as transformation, reduction, scanning and sort.</p></li>
<li><p>The CUDA memory model. CUDA has a wide variety of memory types, differing in their scope, sizes, latency and access pattern. Careful manipulation of memory is key to successful parallel programs. For example.  is only available to threads in the same block, and it is very fast because it is on-chip. On the other hand, the size of shared memory is very limited: at most 64 KB (this is configurable during runtime). Other advanced memory managing techniques include pinned memory, zero-copy memory and so on.</p></li>
</ul>
</div>
<div id="how-fast-is-it" class="section level2">
<h2 class="hasAnchor">
<a href="#how-fast-is-it" class="anchor"></a>How Fast Is It?</h2>
<p>To demonstrate the advantage to GPU-accelerated Gibbs sampler, I compare the MATLAB implementation <span class="citation">Makalic and Schmidt (2016)</span> and my implementation using CUDA. My code is run on a Nvidia Tesla P100 with 3584 CUDA cores and 16 GB GPU memory. The MATLAB implementation is run on my Macbook pro with 3.1 GHz Intel Core i5 and 16GB memory. We can see in all cases CUDA code outperforms the MATLAB implementation, and the advantage becomes increasingly prominent as the data size grows. Each experiment runs for 5000 iterations, from a model with Horseshoe-type sparsity and Gaussian noise.</p>
<center>
<img src="speed.png" style="width: 80%">
</center>
<p>We should also note that, despite exciting results, GPU could not handle super-large datasets due to memory limitation.</p>
</div>
<div id="future-work" class="section level2">
<h2 class="hasAnchor">
<a href="#future-work" class="anchor"></a>Future Work</h2>
<ul>
<li><p>Asynchronous Gibbs sampler suits GPU much better (<span class="math inline">\(x_i^{k+1}\sim f_{i|others}(\cdot|x_1^k,\cdots,x_p^k),\forall i\)</span>), but it requires theoretical justifications.</p></li>
<li><p>Deploy the code across multiple GPUs. For example, a data set of <span class="math inline">\(n = 1,000,000\)</span> and <span class="math inline">\(p = 10,000\)</span>, which takes about 40 GB RAM, already well exceeds the capacity of GPU.</p></li>
</ul>
</div>
<div id="acknowledgement" class="section level2">
<h2 class="hasAnchor">
<a href="#acknowledgement" class="anchor"></a>Acknowledgement</h2>
<p>Thank Dr. Enes Makalic from University of Melbourne for the idea of the project. Thank Google Cloud Platform (GCP) for free education credits to cloud computing service. Finally a huge thankyou to Nvidia engineers for bringing these amazing toys to our world.</p>
<p>Liao acknowledges the support of XiYuan Undergraduate Academic Program of Fudan University. <strong>The project is awarded the Runnerup Award out of around 30 equally competitive posters in the student poster section of <a href="http://www.fbdc.fudan.edu.cn/Data/View/72">2018 Fudan Science and Innovation Forum</a>.</strong></p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-kirk2016programming">
<p>Kirk, David B, and W Hwu Wen-Mei. 2016. <em>Programming Massively Parallel Processors: A Hands-on Approach</em>. Morgan kaufmann.</p>
</div>
<div id="ref-lee2010utility">
<p>Lee, Anthony, Christopher Yau, Michael B Giles, Arnaud Doucet, and Christopher C Holmes. 2010. “On the Utility of Graphics Cards to Perform Massively Parallel Simulation of Advanced Monte Carlo Methods.” <em>Journal of Computational and Graphical Statistics</em> 19 (4). Taylor &amp; Francis: 769–89.</p>
</div>
<div id="ref-makalic2016high">
<p>Makalic, Enes, and Daniel F Schmidt. 2016. “High-Dimensional Bayesian Regularised Regression with the Bayesreg Package.” <em>arXiv Preprint arXiv:1611.06649</em>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#motivation">Motivation</a></li>
      <li><a href="#model-description">Model Description</a></li>
      <li><a href="#a-simulation-study">A Simulation Study</a></li>
      <li><a href="#the-parallel-gibbs-sampler">The Parallel Gibbs Sampler</a></li>
      <li><a href="#gpu-and-cuda">GPU and CUDA</a></li>
      <li><a href="#how-fast-is-it">How Fast Is It?</a></li>
      <li><a href="#future-work">Future Work</a></li>
      <li><a href="#acknowledgement">Acknowledgement</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by <a href="https://www.linkedin.com/in/luofeng-liao-7a1027181/">Luofeng Liao</a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
